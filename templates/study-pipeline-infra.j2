AWSTemplateFormatVersion: '2010-09-09'

Description: The set of Glue jobs created for each study

Parameters:

  Namespace:
    Type: String
    Description: >-
      The namespace string used to build up the path to the correct object keys
      in the bucket
    Default: bridge-downstream

  CodeRepositoryName:
    Type: String
    Description: >-
      Name of code repository, used to build up the path to the correct
      object keys in the bucket
    Default: BridgeDownstream

  AppName:
    Type: String
    Description: App whose data this pipeline infrastructure processes
    AllowedPattern: '[a-z]{1}[a-z0-9.-]*'

  StudyName:
    Type: String
    Description: Study whose data this pipeline infrastructure processes
    AllowedPattern: '[a-z]{1}[a-z0-9.-]*'

  ArtifactRef:
    Type: String
    Description: A branch name or a tag

  RoleArn:
    Type: String
    Description: The ARN of an IAM role that's used to access S3

  ClassifierName:
    Type: String
    Description: Name of the Glue classifier

  JsonToParquetTriggerSchedule:
    Type: String
    Description: >-
      A cron expression for the JSONToParquet Workflow.
      Default is to instead create an on-demand trigger.
    Default: ON_DEMAND

# buckets
  TemplateBucketName:
    Type: String
    Description: Name of the S3 bucket which stores CFN templates

  JsonBucketName:
    Type: String
    Description: Name of the S3 bucket storing json

  JsonPrefix:
    Type: String
    Description: Prefix of the object keys for ndjson data
    Default: raw_json

  ParquetBucketName:
    Type: String
    Description: Name of the S3 bucket where the finished parquet files are stored

  ParquetPrefix:
    Type: String
    Description: Prefix of the object keys for parquet data
    Default: parquet

  SynapseAuthSsmParameterName:
    Type: String
    Description: >-
      The name of an ssm parameter whose value is Synapse service account
      personal access token

  S3ToJsonS3JobName:
    Type: String
    Description: The name of the S3 To JSON S3 Job

  CastGlueDataTypes:
    Type: String
    Description: Whether to manually cast glue data types with resolve choice or not
    AllowedValues:
      - "true"
      - "false"
    Default: "false"

Conditions:
  JsonToParquetTriggerIsNotScheduled: !Equals [!Ref JsonToParquetTriggerSchedule, ON_DEMAND]
  JsonToParquetTriggerIsScheduled: !Not [!Condition JsonToParquetTriggerIsNotScheduled]

Resources:
  # convert crawler assignments to map for ease of lookup
  {% set crawler_assignments = {} %}
  {% for crawler_name, datasets in sceptre_user_data.dataset_crawler_assignments.items() %}
  {% for dataset in datasets %}
  {% do crawler_assignments.update({dataset: crawler_name}) %}
  {% endfor %}
  {% endfor %}

  # combine all configuration data into a "datasets" collection
  {% set datasets = [] %}
  {% for v in sceptre_user_data.dataset_schemas.tables.keys() %}
  {% set dataset = {} %}
  {% do dataset.update({'dataset_name': v}) %}
  {% do dataset.update({'table_name': 'dataset_' + v.lower()})%}
  {% do dataset.update({'s3_partition': 'dataset=' + v})%}
  {% do dataset.update({'crawler': crawler_assignments[v]}) %}
  {% set schema = sceptre_user_data.dataset_schemas.tables[v] %}
  {% do dataset.update({'columns': schema['columns']}) %}
  {% do dataset.update({'partition_keys': schema['partition_keys']}) %}
  {% do dataset.update({'stackname_prefix': '{}'.format(v.replace('_',''))}) %}
  {% do datasets.append(dataset) %}
  {% endfor %}

  # Json to Parquet Job Stacks
  {% for dataset in datasets %}
  {{ dataset['stackname_prefix'] }}ParquetJob:
    Type: AWS::Glue::Job
    Properties:
      Command:
        Name: glueetl
        ScriptLocation: !Sub s3://${TemplateBucketName}/${CodeRepositoryName}/${ArtifactRef}/glue/jobs/json_s3_to_parquet.py
      DefaultArguments:
        --TempDir: !Sub s3://${JsonBucketName}/tmp
        --enable-continuous-cloudwatch-log: true
        --enable-metrics: true
        --enable-spark-ui: true
        --spark-event-logs-path: !Sub s3://${JsonBucketName}/spark-logs/${AWS::StackName}/
        --job-bookmark-option: job-bookmark-enable
        --job-language: python
        --table: {{ '{}'.format(dataset['table_name']) }}
        --cast-glue-data-types: !Sub CastGlueDataTypes
        # --conf spark.sql.adaptive.enabled
      Description: {{ 'Export {} data in parquet format'.format(dataset['dataset_name']) }}
      ExecutionProperty:
        MaxConcurrentRuns: 1
      GlueVersion: '3.0' # Spark 3.1.1, Python 3.7 # TODO: parameterize
      MaxRetries: 0 # change this when not in development; TODO: parameterize
      Name: !Sub '${Namespace}-${AppName}-${StudyName}-{{dataset['stackname_prefix']}}-Job'
      NumberOfWorkers: 1
      Role: !Ref RoleArn
      Timeout: 120
      WorkerType: Standard
  {% endfor %}

  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Description: !Sub 'BridgeDownstream database for ${AppName} ${StudyName}'

  # Table stacks
  {% for dataset in datasets %}
  {{ dataset.stackname_prefix }}Table:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: {{ dataset.table_name }}
        Parameters:
          CrawlerSchemaDeserializerVersion: '1.0'
          CrawlerSchemaSerializerVersion: '1.0'
          classification: json
          compressionType: none
          typeOfData: file
          {% if dataset.crawler == 'array_of_records' %}
          jsonPath: '$[*]'
          {% endif %}
        PartitionKeys: {{ dataset.partition_keys }}
        Retention: 0
        StorageDescriptor:
          Columns: {{ dataset.columns }}
          Compressed: false
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          Location: !Sub s3://${JsonBucketName}/${Namespace}/${AppName}/${StudyName}/${JsonPrefix}/{{dataset.s3_partition}}/
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          StoredAsSubDirectories: false
        TableType: EXTERNAL_TABLE
    {% endfor %}

  StandardCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Configuration: '{"Version":1.0,"CrawlerOutput":{"Partitions":{"AddOrUpdateBehavior":"InheritFromTable"}},"Grouping":{"TableGroupingPolicy":"CombineCompatibleSchemas"}}'
      DatabaseName: !Ref GlueDatabase
      Name: !Sub ${Namespace}-${AppName}-${StudyName}-standard
      RecrawlPolicy:
        RecrawlBehavior: CRAWL_NEW_FOLDERS_ONLY
      Role: !Ref RoleArn
      SchemaChangePolicy:
        DeleteBehavior: LOG
        UpdateBehavior: LOG
      Targets:
        S3Targets:
          {% for dataset in datasets %}
          {% if dataset.crawler == 'standard' %}
          - Path: !Sub s3://${JsonBucketName}/${Namespace}/${AppName}/${StudyName}/${JsonPrefix}/{{dataset.s3_partition}}/
          {% endif %}
          {% endfor %}

  ArrayOfRecordsCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Classifiers:
        - !Ref ClassifierName
      Configuration: '{"Version":1.0,"CrawlerOutput":{"Partitions":{"AddOrUpdateBehavior":"InheritFromTable"}},"Grouping":{"TableGroupingPolicy":"CombineCompatibleSchemas"}}'
      DatabaseName: !Ref GlueDatabase
      Name: !Sub ${Namespace}-${AppName}-${StudyName}-array-of-records
      RecrawlPolicy:
        RecrawlBehavior: CRAWL_NEW_FOLDERS_ONLY
      Role: !Ref RoleArn
      SchemaChangePolicy:
        DeleteBehavior: LOG
        UpdateBehavior: LOG
      Targets:
        S3Targets:
          {% for dataset in datasets %}
          {% if dataset.crawler == 'array_of_records' %}
          - Path: !Sub s3://${JsonBucketName}/${Namespace}/${AppName}/${StudyName}/${JsonPrefix}/{{dataset.s3_partition}}/
          {% endif %}
          {% endfor %}

  S3ToJsonWorkflow:
    Type: AWS::Glue::Workflow
    Properties:
      DefaultRunProperties:
        namespace: !Ref Namespace
        app_name: !Ref AppName
        study_name: !Ref StudyName
        json_bucket: !Ref JsonBucketName
        json_prefix: !Ref JsonPrefix
      Description: >-
        Workflow that breaks apart an archive and stores individual files in S3
      Name: !Sub ${Namespace}-${AppName}-${StudyName}-S3ToJsonWorkflow

  JsonToParquetWorkflow:
    Type: AWS::Glue::Workflow
    Properties:
      DefaultRunProperties:
        app_name: !Ref AppName
        study_name: !Ref StudyName
        database: !Ref GlueDatabase
        parquet_bucket: !Ref ParquetBucketName
        parquet_prefix: !Sub ${Namespace}/${AppName}/${StudyName}/${ParquetPrefix}
      Description: Workflow for converting json to parquet
      Name: !Sub ${Namespace}-${AppName}-${StudyName}-JsonToParquetWorkflow

  NewDataTrigger:
    Type: AWS::Glue::Trigger
    Properties:
      Actions:
        - JobName: !Ref S3ToJsonS3JobName
      Description: >-
        When new data is received this trigger starts the workflow
        that unpacks the archive and stores JSON files separately
      Type: ON_DEMAND
      WorkflowName: !Ref S3ToJsonWorkflow

  JsonToParquetTriggerOnDemand:
    Condition: JsonToParquetTriggerIsNotScheduled
    Type: AWS::Glue::Trigger
    Properties:
      Actions:
        - CrawlerName: !Ref StandardCrawler
        - CrawlerName: !Ref ArrayOfRecordsCrawler
      Description: Starts crawlers for the JSON to Parquet workflow
      Type: ON_DEMAND
      WorkflowName: !Ref JsonToParquetWorkflow

  JsonToParquetTriggerScheduled:
    Condition: JsonToParquetTriggerIsScheduled
    Type: AWS::Glue::Trigger
    Properties:
      Actions:
        - CrawlerName: !Ref StandardCrawler
        - CrawlerName: !Ref ArrayOfRecordsCrawler
      Description: Starts crawlers for the JSON to Parquet workflow
      Type: SCHEDULED
      Schedule: !Ref JsonToParquetTriggerSchedule
      StartOnCreation: true
      WorkflowName: !Ref JsonToParquetWorkflow

  JsonCrawlersDoneTrigger:
    Type: AWS::Glue::Trigger
    Properties:
      Actions:
        {% for dataset in datasets %}
        - JobName: !Ref {{dataset.stackname_prefix}}ParquetJob
        {% endfor %}
      Predicate:
        Conditions:
        - CrawlState: SUCCEEDED
          CrawlerName: !Ref StandardCrawler
          LogicalOperator: EQUALS
        - CrawlState: SUCCEEDED
          CrawlerName: !Ref ArrayOfRecordsCrawler
          LogicalOperator: EQUALS
        Logical: AND
      StartOnCreation: true
      Type: CONDITIONAL
      WorkflowName: !Ref JsonToParquetWorkflow
