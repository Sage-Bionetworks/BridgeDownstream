AWSTemplateFormatVersion: '2010-09-09'

Description: The set of Glue jobs created for each study

Parameters:

  Namespace:
    Type: String
    Description: >-
      The namespace string used to build up the path to the correct object keys
      in the bucket
    Default: main

  CodeRepositoryName:
    Type: String
    Description: >-
      Name of code repository, used to build up the path to the correct
      object keys in the bucket
    Default: BridgeDownstream

  AppName:
    Type: String
    Description: App whose data this pipeline infrastructure processes
    AllowedPattern: '[a-z]{1}[a-z0-9.-]*'

  StudyName:
    Type: String
    Description: Study whose data this pipeline infrastructure processes
    AllowedPattern: '[a-z]{1}[a-z0-9.-]*'

  RoleArn:
    Type: String
    Description: The ARN of an IAM role that's used to access S3

  ClassifierName:
    Type: String
    Description: Name of the Glue classifier

# buckets
  TemplateBucketName:
    Type: String
    Description: Name of the S3 bucket which stores CFN templates

  JsonBucketName:
    Type: String
    Description: Name of the S3 bucket storing json

  JsonPrefix:
    Type: String
    Description: Prefix of the object keys for ndjson data
    Default: raw_json

  ParquetBucketName:
    Type: String
    Description: Name of the S3 bucket where the finished parquet files are stored

  ParquetPrefix:
    Type: String
    Description: Prefix of the object keys for parquet data
    Default: parquet

  # TODO: review use of SourceBucketName
  # temporary: this will be replaced with the sns lambda that pulls data from a bridge bucket
  SourceBucketName:
    Type: String
    Description: Name of the S3 bucket containing source data

  SynapseAuthSsmParameterName:
    Type: String
    Description: >-
      The name of an ssm parameter whose value is Synapse service account
      personal access token

  UniqueId:
    Type: String
    Description: A unique id for producing unique job names.
    Default: ''

  S3ToJsonS3JobName:
    Type: String
    Description: The name of the S3 To JSON S3 Job

Resources:
  # convert crawler assignments to map for ease of lookup
  {% set crawler_assignments = {} %}
  {% for crawler_name, datasets in sceptre_user_data.dataset_crawler_assignments.items() %}
  {% for dataset in datasets %}
  {% do crawler_assignments.update({dataset: crawler_name}) %}
  {% endfor %}
  {% endfor %}

  # combine all configuration data into a "datasets" collection
  {% set datasets = [] %}
  {% for k,v in sceptre_user_data.dataset_version_mapping['appVersion'][sceptre_user_data.app_version]['dataset'].items() %}
  {% set dataset_name = '{}_{}'.format(k,v) %}
  {% set dataset = {} %}
  {% do dataset.update({'dataset_name': dataset_name}) %}
  {% do dataset.update({'table_name': 'dataset_' + dataset_name})%}
  {% do dataset.update({'s3_partition': 'dataset=' + dataset_name})%}
  {% do dataset.update({'crawler': crawler_assignments[dataset_name]}) %}
  {% set schema = sceptre_user_data.dataset_schemas.tables[dataset_name] %}
  {% do dataset.update({'columns': schema['columns']}) %}
  {% do dataset.update({'partition_keys': schema['partition_keys']}) %}
  {% do dataset.update({'stackname_prefix': '{}{}'.format(k.replace('_','').capitalize(), v.capitalize())}) %}
  {% do datasets.append(dataset) %}
  {% endfor %}

  # Json to Parquet Job Stacks
  {% for dataset in datasets %}
  {{ dataset['stackname_prefix'] }}ParquetJob:
    Type: AWS::Glue::Job
    Properties:
      Command:
        Name: glueetl
        ScriptLocation: !Sub s3://${TemplateBucketName}/${CodeRepositoryName}/${Namespace}/glue/jobs/json_s3_to_parquet.py
      DefaultArguments:
        --TempDir: !Sub s3://${JsonBucketName}/tmp
        --enable-continuous-cloudwatch-log: true
        --enable-metrics: true
        --enable-spark-ui: true
        --spark-event-logs-path: !Sub s3://${JsonBucketName}/spark-logs/${AWS::StackName}/
        --job-bookmark-option: job-bookmark-enable
        --job-language: python
        --table: {{ '{}'.format(dataset['table_name']) }}
        # --conf spark.sql.adaptive.enabled
      Description: {{ 'Export {} data in parquet format'.format(dataset['dataset_name']) }}
      ExecutionProperty:
        MaxConcurrentRuns: 1
      GlueVersion: '3.0' # Spark 3.1.1, Python 3.7 # parameterize?
      MaxRetries: 0 # change this when not in development; parameterize?
      Name: !Sub '${AppName}-${StudyName}-{{dataset['stackname_prefix']}}-Job-${UniqueId}'
      NumberOfWorkers: 1
      Role: !Ref RoleArn
      Timeout: 120
      WorkerType: Standard
  {% endfor %}

  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Description: !Sub 'BridgeDownstream database for ${AppName} ${StudyName}'

  # Table stacks
  {% for dataset in datasets %}
  {{ dataset.stackname_prefix }}Table:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: {{ dataset.table_name }}
        Parameters:
          CrawlerSchemaDeserializerVersion: '1.0'
          CrawlerSchemaSerializerVersion: '1.0'
          classification: json
          compressionType: none
          typeOfData: file
          {% if dataset.crawler == 'array_of_records' %}
          jsonPath: '$[*]'
          {% endif %}
        PartitionKeys: {{ dataset.partition_keys }}
        Retention: 0
        StorageDescriptor:
          Columns: {{ dataset.columns }}
          Compressed: false
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          Location: !Sub s3://${JsonBucketName}/${AppName}/${StudyName}/${JsonPrefix}/{{dataset.s3_partition}}/
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          StoredAsSubDirectories: false
        TableType: EXTERNAL_TABLE
    {% endfor %}

  StandardCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Configuration: '{"Version":1.0,"CrawlerOutput":{"Partitions":{"AddOrUpdateBehavior":"InheritFromTable"}},"Grouping":{"TableGroupingPolicy":"CombineCompatibleSchemas"}}'
      DatabaseName: !Ref GlueDatabase
      Name: !Sub ${AppName}-${StudyName}-standard
      RecrawlPolicy:
        RecrawlBehavior: CRAWL_NEW_FOLDERS_ONLY
      Role: !Ref RoleArn
      SchemaChangePolicy:
        DeleteBehavior: LOG
        UpdateBehavior: LOG
      Targets:
        S3Targets:
          {% for dataset in datasets %}
          {% if dataset.crawler == 'standard' %}
          - Path: !Sub s3://${JsonBucketName}/${AppName}/${StudyName}/${JsonPrefix}/{{dataset.s3_partition}}/
          {% endif %}
          {% endfor %}

  ArrayOfRecordsCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Classifiers:
        - !Ref ClassifierName
      Configuration: '{"Version":1.0,"CrawlerOutput":{"Partitions":{"AddOrUpdateBehavior":"InheritFromTable"}},"Grouping":{"TableGroupingPolicy":"CombineCompatibleSchemas"}}'
      DatabaseName: !Ref GlueDatabase
      Name: !Sub ${AppName}-${StudyName}-array-of-records
      RecrawlPolicy:
        RecrawlBehavior: CRAWL_NEW_FOLDERS_ONLY
      Role: !Ref RoleArn
      SchemaChangePolicy:
        DeleteBehavior: LOG
        UpdateBehavior: LOG
      Targets:
        S3Targets:
          {% for dataset in datasets %}
          {% if dataset.crawler == 'array_of_records' %}
          - Path: !Sub s3://${JsonBucketName}/${AppName}/${StudyName}/${JsonPrefix}/{{dataset.s3_partition}}/
          {% endif %}
          {% endfor %}

  WorkflowsStack:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Sub https://${TemplateBucketName}.s3.amazonaws.com/${CodeRepositoryName}/${Namespace}/templates/glue-workflows.yaml
      Parameters:
        AppName: !Ref AppName
        StudyName: !Ref StudyName
        DatabaseName: !Ref GlueDatabase
        JsonBucketName: !Ref JsonBucketName
        ParquetBucketName: !Ref ParquetBucketName
        SourceBucketName: !Ref SourceBucketName

  NewDataTrigger:
    Type: AWS::Glue::Trigger
    Properties:
      Actions:
        - JobName: !Ref S3ToJsonS3JobName
      Description: >-
        When new data is received this trigger starts the workflow
        that unpacks the archive and stores JSON files separately
      Type: ON_DEMAND
      WorkflowName: !Sub ${WorkflowsStack.Outputs.S3ToJsonWorkflowName}

  JsonToParquetTrigger:
    Type: AWS::Glue::Trigger
    Properties:
      Actions:
        - CrawlerName: !Ref StandardCrawler
        - CrawlerName: !Ref ArrayOfRecordsCrawler
      Description: Starts crawlers for the JSON to Parquet workflow
      Type: ON_DEMAND
      WorkflowName: !Sub ${WorkflowsStack.Outputs.JsonToParquetWorkflowName}

  JsonCrawlersDoneTrigger:
    Type: AWS::Glue::Trigger
    Properties:
      Actions:
        {% for dataset in datasets %}
        - JobName: !Ref {{dataset.stackname_prefix}}ParquetJob
        {% endfor %}
      Predicate:
        Conditions:
        - CrawlState: SUCCEEDED
          CrawlerName: !Ref StandardCrawler
          LogicalOperator: EQUALS
        - CrawlState: SUCCEEDED
          CrawlerName: !Ref ArrayOfRecordsCrawler
          LogicalOperator: EQUALS
        Logical: AND
      StartOnCreation: true
      Type: CONDITIONAL
      WorkflowName: !Sub ${WorkflowsStack.Outputs.JsonToParquetWorkflowName}

  LambdaStack:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Sub https://${TemplateBucketName}.s3.amazonaws.com/${CodeRepositoryName}/${Namespace}/templates/lambda/sns_to_glue/template.yaml
      Parameters:
        WorkflowName: !Sub ${WorkflowsStack.Outputs.S3ToJsonWorkflowName}
